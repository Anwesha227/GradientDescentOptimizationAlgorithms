# GradientDescentOptimizationAlgorithms

I shall discuss and implement some of the variations of the gradient descent algorithm and optimization methods. 
If you have some idea of what Gradient Descent is and would like to explore more, this might be useful to you.

Here's a quick (unfinished) list of what wthis notebook will cover :

 - Vanilla (Batch) Gradient Descent
 - Stochastic Gradient Descent
 - Mini Batch Gradient Descent
 - Momentum
 - Nesterov Accelerated Gradient
 - Adagrad
 - RMSProp
 
Here are the other algorithms that will be covered in future updates :

 - AdaMax
 - Adam
 - Adadelta
 - Nadam and possibly some others.
