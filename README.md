# GradientDescentOptimizationAlgorithms

I shall discuss and implement some of the variations of the gradient descent algorithm and optimization methods from scratch, with some brief explanations and intuitions. The 3D visulaizations make use of Plotly.
If you have some idea of what Gradient Descent is and would like to explore more, this might be useful to you.

Here's a quick (unfinished) list of what this notebook will cover :

 - Vanilla (Batch) Gradient Descent
 - Stochastic Gradient Descent
 - Mini Batch Gradient Descent
 - Momentum
 - Nesterov Accelerated Gradient
 - Adagrad
 - RMSProp
 
Here are the other algorithms that will be covered in future updates :

 - AdaMax
 - Adam
 - Adadelta
 - Nadam and possibly some others.


## Note :
If you are not able to view the notebook on GitHub and are getting the message "Sorry, this is too big to display.", please follow these steps : 
 - Head to [Nbviewer](https://nbviewer.org/)
 - Copy and Paste the following URL : https://github.com/Anwesha227/GradientDescentOptimizationAlgorithms/blob/main/gradient-descent-optimization-algorithms.ipynb
 - Hit "Go!"
 
 You should be able to view the Notebook and the interactive plots after this.
